{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99066ced-5dfe-4622-a335-491315cb672a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Aggregations\n",
    "- summary() and describe() are aggregations as well; returns descriptive statistics\n",
    "- agg, avg, count, max, sum, groupBy, approx_count_distict etc.\n",
    "\n",
    "Anytime, an aggregation is done, it is done in 2 steps: (1) collecting data from memory partition, (2) performing final sum operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f74ee6-b047-437e-8c6c-0c852dd91c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/simon.nagy@hiflylabs.com/.bundle/dbxSandbox/dbxSandbox/sandboxProject/includeFiles/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfa05d2-b206-4782-89a6-d8d73f58183e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(events_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c93041-98bc-48ed-9734-98b48756fe9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"event_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96ace8a-697d-48b0-8831-bc2d54adc8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_df = df.groupBy(\"geo.state\", \"geo.city\").count().orderBy(\"count\", ascending=False)\n",
    "# take multiple rows, and aggregate them into a single row\n",
    "# result set is performed, when first aggregatio is done <= in this case count()\n",
    "# count can be replaced by agg, avg, sum etc. of course\n",
    "# other operations can be preformed as alias columns etc.\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d5916e-93e8-4eed-8cca-b1491aa04193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# approx_count_distinct <= very large python objects can be counted with this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5a2a8c-d67c-49d2-a5d6-15a24e81f75d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "state_purchases = df.groupBy(\"geo.state\").agg(sum(\"ecommerce.total_item_quantity\").alias(\"state_purchases\")).orderBy(\"state_purchases\", ascending=False)\n",
    "display(state_purchases)\n",
    "\n",
    "# when performing an alias, \"agg\" is required (not only sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d28491c-cbe2-4950-b185-7cc9354cd27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, approx_count_distinct\n",
    "\n",
    "state_aggregated_df = (\n",
    "    df\n",
    "    .groupBy(\"geo.state\")\n",
    "    .agg(avg(\"ecommerce.total_item_quantity\").alias(\"avg_quantity\"),\n",
    "        approx_count_distinct(\"user_id\").alias(\"distinct_users\"))\n",
    "    .orderBy(\"avg_quantity\", ascending=False)\n",
    ")\n",
    "\n",
    "display(state_aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c99ca81-abfd-48ea-95e5-7b2767d8d5cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classroom_cleanup()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Functions-1-aggregations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
